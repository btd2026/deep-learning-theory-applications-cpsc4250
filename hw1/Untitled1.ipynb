{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c147d-253e-40fc-92de-3bf194f7fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Problem 5: Convolutional Neural Networks (PyTorch Implementation)\n",
    "\n",
    "Time to implement your first convolutional neural network (CNN) in PyTorch!\n",
    "\n",
    "For this assignment, we'll be training the network on the canonical MNIST dataset. After building the network, we'll experiment with an array of hyperparameters, tweaking the network's width, depth, learning rate and more in pursuit of the highest classification accuracy we can muster.\n",
    "\n",
    "You may find the PyTorch tutorials helpful as you complete this problem: https://pytorch.org/tutorials/beginner/basics/intro.html. If you haven't yet, we suggest you go through them. Pay more attention to the tutorial on the optimization loop, which you will need to build more or less from scratch.\n",
    "### Step 0: Setup Environment\n",
    "If you haven't set up PyTorch locally, you can do so following this [local installation guide](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "\n",
    "Installing PyTorch locally is **not** necessary for the course. You can access PyTorch either through:\n",
    "\n",
    "- the class partition `cpsc452` on [McCleary](https://docs.ycrc.yale.edu/clusters/mccleary/)\n",
    "\n",
    "- use of [Google Colab](https://colab.research.google.com)\n",
    "\n",
    "If you are new to the Yale High Performance Clusters (HPC) please consulte this [guide](https://docs.ycrc.yale.edu/clusters-at-yale/)\n",
    "<div style=\"display:none\">\n",
    "\n",
    "```bash\n",
    "[mccleary ~]$ salloc ---reservation=cpsc452\n",
    "[cpsc452_netID@gpu ~]$  bash\n",
    "```\n",
    "\n",
    "```bash\n",
    "# sbatch.script\n",
    "@SBATCH -p cpsc452\n",
    "```\n",
    "\n",
    "As usual, we'll start by importing the necessary libraries and setting up our environment. Please run the following cell to do so.\n",
    "!pip install numpy matplotlib tqdm\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn            # neural network modules\n",
    "import torch.nn.functional as F  # activation functions\n",
    "import torch.optim as optim      # optimizer\n",
    "import torch.utils.data          # dataloader\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Download the MNIST dataset\n",
    "mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=None)\n",
    "mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
    "\n",
    "# Load into torch datasets\n",
    "train_dataset = torch.utils.data.TensorDataset(mnist_train.data.unsqueeze(1).float(), mnist_train.targets.long())\n",
    "test_dataset = torch.utils.data.TensorDataset(mnist_test.data.unsqueeze(1).float(), mnist_test.targets.long())\n",
    "\n",
    "# Visualize the data\n",
    "for i in range(100):\n",
    "    plt.subplot(10, 10, i+1)\n",
    "    plt.imshow(train_dataset[i][0][0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "### Step 1: Learn PyTorch Basics\n",
    "In this section, you will learn different PyTorch basic operations (`Conv2d`, `MaxPool`, `Linear`) and reshape operations. You might refer to PyTorch documentation for details of these operations.\n",
    "# Part 1: Explore `nn.Module`\n",
    "image = torch.randn(1, 1, 28, 28)  # image: (1, 1, 28, 28)\n",
    "\n",
    "\n",
    "# TODO: define a 3x3 convolutional layer that maps 1 input channel to 32 output channels\n",
    "# refer to https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "# you might need to specify the input channels, output channels, kernel size, stride, padding, etc.\n",
    "conv_1 = ...\n",
    "output_1 = conv_1(image)    # image: (1, 1, 28, 28) -> output_1: (1, 32, 28, 28)\n",
    "assert output_1.shape == (1, 32, 28, 28), \"The shape of output_1 is incorrect!\"\n",
    "\n",
    "\n",
    "# TODO: define a max pooling layer that halves the height and width of the input\n",
    "# refer to https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
    "# you might need to specify the kernel size, stride, padding, etc.\n",
    "pool_1 = ...\n",
    "output_2 = pool_1(output_1) # output_1: (1, 32, 28, 28) -> output_2: (1, 32, 14, 14)\n",
    "assert output_2.shape == (1, 32, 14, 14), \"The shape of output_2 is incorrect!\"\n",
    "\n",
    "\n",
    "# TODO: define a 3x3 convolutional layer that maps 32 input channels to 64 output channels\n",
    "conv_2 = ...\n",
    "output_3 = conv_2(output_2) # output_2: (1, 32, 14, 14) -> output_3: (1, 64, 14, 14)\n",
    "assert output_3.shape == (1, 64, 14, 14), \"The shape of output_3 is incorrect!\"\n",
    "\n",
    "\n",
    "# TODO: define a max pooling layer that halves the height and width of the input\n",
    "pool_2 = ...\n",
    "output_4 = pool_2(output_3) # output_3: (1, 64, 14, 14) -> output_4: (1, 64, 7, 7)\n",
    "assert output_4.shape == (1, 64, 7, 7), \"The shape of output_4 is incorrect!\"\n",
    "\n",
    "\n",
    "# TODO: flatten the output of the previous layer\n",
    "# refer to https://pytorch.org/docs/stable/generated/torch.flatten.html\n",
    "flatten_4 = ...            # output_4: (1, 64, 7, 7) -> flatten_4: (1, 64 * 7 * 7)\n",
    "assert flatten_4.shape == (1, 64 * 7 * 7), \"The shape of flatten_4 is incorrect!\"\n",
    "\n",
    "\n",
    "# TODO: define a linear layer that maps 64 * 7 * 7 input features to 10 output features\n",
    "# refer to https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "# you might need to specify the input size, output size, etc.\n",
    "fc = ...\n",
    "output_5 = fc(flatten_4)     # flatten_4: (1, 64 * 7 * 7) -> output_5: (1, 10)\n",
    "assert output_5.shape == (1, 10), \"The shape of output_5 is incorrect!\"\n",
    "\n",
    "\n",
    "# Part 2: Explore reshape, squeeze, unsqueeze, transpose, repeat\n",
    "tensor = torch.tensor([[[1, 2, 3, 4], [5, 6, 7, 8]]])  # tensor: (1, 2, 4)\n",
    "\n",
    "# TODO: reshape the tensor to (2, 2, 2)\n",
    "# refer to https://pytorch.org/docs/stable/generated/torch.reshape.html\n",
    "reshaped_tensor = ...\n",
    "assert torch.allclose(reshaped_tensor, torch.tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])), \"The reshaped tensor is incorrect!\"\n",
    "\n",
    "\n",
    "# TODO: squeeze the first dimension of the tensor\n",
    "# refer to https://pytorch.org/docs/stable/generated/torch.squeeze.html\n",
    "squeezed_tensor = ...\n",
    "assert torch.allclose(squeezed_tensor, torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])), \"The squeezed tensor is incorrect!\"\n",
    "\n",
    "\n",
    "# TODO: unsqueeze the first dimension of the tensor\n",
    "# refer to https://pytorch.org/docs/stable/generated/torch.unsqueeze.html\n",
    "unsqueeze_tensor = ...\n",
    "assert torch.allclose(unsqueeze_tensor, torch.tensor([[[[1, 2, 3, 4], [5, 6, 7, 8]]]])), \"The unsqueezed tensor is incorrect!\"\n",
    "\n",
    "\n",
    "# TODO: transpose dim 1 and dim 2 of the tensor\n",
    "# refer to https://pytorch.org/docs/stable/generated/torch.transpose.html\n",
    "transposed_tensor = ...\n",
    "assert torch.allclose(transposed_tensor, torch.tensor([[[1, 5], [2, 6], [3, 7], [4, 8]]])), \"The transposed tensor is incorrect!\"\n",
    "\n",
    "\n",
    "# TODO: repeat the tensor 3 times along dim 0\n",
    "# refer to https://pytorch.org/docs/stable/generated/torch.repeat.html\n",
    "repeated_tensor = ...\n",
    "assert torch.allclose(repeated_tensor, torch.tensor([[[1, 2, 3, 4], [5, 6, 7, 8]], [[1, 2, 3, 4], [5, 6, 7, 8]], [[1, 2, 3, 4], [5, 6, 7, 8]]])), \"The repeated tensor is incorrect!\"\n",
    "### Step 2: Build and Train a SimpleCNN on MNIST Dataset\n",
    "Follow the TODOs to build a two-layer fully-connected neural network. This is the first ``SimpleCNN`` with linear layers only. You will use this as a baseline model for the next step.\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 1,\n",
    "        output_dim: int = 10,\n",
    "        hidden_dim_list: list = [4, 8],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim_list = hidden_dim_list\n",
    "\n",
    "        # TODO: define the layers of the network\n",
    "        self.conv_1 = ...\n",
    "        self.conv_2 = ...\n",
    "        self.fc = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = ...             # TODO: flatten the output of the previous layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "Implement the training function for your CNN. The function should take the model, optimizer, loss function, training data loader, and validation data loader as input. It should return the training and validation loss and accuracy after each epoch.\n",
    "\n",
    "Implement the ``plot_metrics`` function to visualize the training history.\n",
    "\n",
    "**Warning**: When implementing the training loop, be aware that in each iteration, the `loss` variable is a tensor. It's important to extract its scalar value for logging or calculating average loss. Use `loss.item()` to get the scalar value of the tensor. Otherwise, you might encounter unexpected out-of-memory errors.\n",
    "def plot_metrics(train_metrics, test_metrics, xlabel, ylabel, title):\n",
    "    # TODO: plot train and test metrics in a single plot\n",
    "    raise NotImplementedError()\n",
    "\n",
    "def train(model, loss_fn, train_loader, test_loader, optimizer, epochs=5):\n",
    "    \"\"\"Train the model.\n",
    "    Args:\n",
    "        model: the model\n",
    "        loss_fn: the loss function\n",
    "        train_loader: the training data loader\n",
    "        test_loader: the testing data loader\n",
    "        optimizer: the optimizer\n",
    "        epochs: the number of epochs to train\n",
    "    Returns:\n",
    "        train_losses: the training losses\n",
    "        test_losses: the testing losses\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    loop = tqdm.tqdm(range(1, epochs + 1))\n",
    "\n",
    "    for epoch in loop:\n",
    "        # TODO: implement training and testing loop\n",
    "\n",
    "        # train the model for one epoch\n",
    "        train_loss, train_accuracy = ...\n",
    "        ...\n",
    "\n",
    "        # test the model for one epoch\n",
    "        test_loss, test_accuracy = ...\n",
    "        ...\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(train_loss=train_loss, test_loss=test_loss, train_accuracy=train_accuracy, test_accuracy=test_accuracy)\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "\n",
    "def train_epoch(model, loss_fn, train_loader, optimizer):\n",
    "    \"\"\"Train the model for one epoch.\n",
    "    Args:\n",
    "        model: the model\n",
    "        loss_fn: the loss function\n",
    "        train_loader: the training data loader\n",
    "        optimizer: the optimizer\n",
    "    Returns:\n",
    "        train_loss: the loss of the epoch\n",
    "    \"\"\"\n",
    "    model.train()  # set model to training mode\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # TODO: implement training iteration\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def test_epoch(model, loss_fn, test_loader):\n",
    "    \"\"\"Test the model for one epoch.\n",
    "    Args:\n",
    "        model: the model\n",
    "        loss_fn: the loss function\n",
    "        test_loader: the testing data loader\n",
    "    Returns:\n",
    "        test_loss: the loss of the epoch\n",
    "    \"\"\"\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    test_loss = 0\n",
    "    test_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():  # disable gradient calculation\n",
    "        for data, target in test_loader:\n",
    "            # TODO: implement test iteration\n",
    "            raise NotImplementedError\n",
    "\n",
    "    return test_loss, test_accuracy\n",
    "Use the training function above to train your ``SimpleCNN`` on ``MNIST`` dataset. You should get a training accuracy less than 92%. Don't worry, we will improve it in the next step.\n",
    "\n",
    "Here are some hyperparameters you can try to improve the performance of your model (we will dive into hyperparameter tuning in the last step):\n",
    "- Number of hidden units\n",
    "- Learning rate\n",
    "- Number of training epochs\n",
    "- Batch size\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "epochs = 10\n",
    "input_dim = 1\n",
    "hidden_dim_list = [4, 8]\n",
    "output_dim = ...    # TODO: define the output dimension\n",
    "\n",
    "model = ...\n",
    "loss_fn = ...       # refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "optimizer = ...     # refer to https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
    "\n",
    "train_loader = ...  # refer to https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "test_loader = ...   # refer to https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = train(model, loss_fn, train_loader, test_loader, optimizer, epochs=epochs)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_metrics(train_losses, test_losses, xlabel=\"Epoch\", ylabel=\"Loss\", title=\"Loss\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_metrics(train_accuracies, test_accuracies, xlabel=\"Epoch\", ylabel=\"Accuracy\", title=\"Accuracy\")\n",
    "### Step 3: Improve the SimpleCNN\n",
    "As you can see in the previous step, the training accuracy of the ``SimpleCNN`` is poor. In this step, you will improve the performance of the ``SimpleCNN`` by adding ``nn.MaxPool2d``, ``nn.Dropout``, and activation functions.\n",
    "\n",
    "**Hint**: The max pooling layer is used to downsample the input along the spatial dimensions (width and height) independently for each channel. It is recommended to add the max pooling layer after the activation function.\n",
    "class CNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 1,\n",
    "        output_dim: int = 10,\n",
    "        hidden_dim_list: list = [4, 8],\n",
    "        p: float = 0.0,\n",
    "        act_fn: Callable = F.relu,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim_list = hidden_dim_list\n",
    "\n",
    "        # TODO: define the layers of the network\n",
    "        self.conv_1 = ...\n",
    "        self.pool_1 = ...\n",
    "        self.conv_2 = ...\n",
    "        self.pool_2 = ...\n",
    "        self.fc = ...\n",
    "        self.act_fn = ...\n",
    "        self.dropout = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: add activation functions and dropout to the correct layers\n",
    "        x = self.conv_1(x)\n",
    "        x = self.pool_1(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.pool_2(x)\n",
    "        x = ...             # TODO: flatten the output of the previous layer\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "Again, use the training function and the same set of hyperparameters above to train your ``CNN`` on ``MNIST dataset``. You should get a training accuracy around 95%.\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "epochs = 10\n",
    "input_dim = 1\n",
    "hidden_dim_list = [4, 8]\n",
    "output_dim = ...    # TODO: define the output dimension\n",
    "act_fn = F.relu\n",
    "p = 0.0\n",
    "\n",
    "model = ...\n",
    "loss_fn = ...       # refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "optimizer = ...     # refer to https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
    "\n",
    "train_loader = ...  # refer to https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "test_loader = ...   # refer to https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = train(model, loss_fn, train_loader, test_loader, optimizer, epochs=epochs)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plot_metrics(train_losses, test_losses, xlabel=\"Epoch\", ylabel=\"Loss\", title=\"Loss\")\n",
    "plt.subplot(2, 1, 2)\n",
    "plot_metrics(train_accuracies, test_accuracies, xlabel=\"Epoch\", ylabel=\"Accuracy\", title=\"Accuracy\")\n",
    "Here are the experiments:\n",
    "- Try adjusting the learning rate to improve its accuracy. You might also try increasing the number of epochs used. Record your results in a table.\n",
    "- Try training your network with different non-linearities between the layers (i.e. relu, softplus, elu, tanh). You should experiment with these and record your test results for each in a table\n",
    "- Try changing the width of the hidden layer, keeping the activation function that performs best. Remember to add these results to your table.\n",
    "- Experiment with the optimizer of your network (i.e. SGD, Adam, RMSProp). You should experiment with these and record your test results for each in a table\n",
    "batch_size = 64\n",
    "learning_rate = ...\n",
    "epochs = ...\n",
    "input_dim = 1\n",
    "hidden_dim_list = [..., ...]\n",
    "output_dim = ...    # TODO: define the output dimension\n",
    "act_fn = ...        # TODO: define the activation function\n",
    "p = ...             # TODO: define the dropout probability\n",
    "\n",
    "model = ...\n",
    "loss_fn = ...       # refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "optimizer = ...     # refer to https://pytorch.org/docs/stable/generated/torch.optim.SGD.html\n",
    "\n",
    "train_loader = ...  # refer to https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "test_loader = ...   # refer to https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = train(model, loss_fn, train_loader, test_loader, optimizer, epochs=epochs)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_metrics(train_losses, test_losses, xlabel=\"Epoch\", ylabel=\"Loss\", title=\"Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_metrics(train_accuracies, test_accuracies, xlabel=\"Epoch\", ylabel=\"Accuracy\", title=\"Accuracy\")\n",
    "### Step 4: Hyperparameter Tuning\n",
    "Now the interesting part begins. Try to improve the performance of your ``CNN`` by tuning the hyperparameters. You should be able to get a training accuracy around 98% and a validation accuracy around 97%.\n",
    "\n",
    "Here are some new parameters you can try to improve the performance of your model:\n",
    "- ``Optimizer (SGD, Adam, RMSProp, etc)``: Different optimizers may lead to different convergence speed and performance.\n",
    "- ``Weight decay (L2 penalty)``: Weight decay is a regularization technique to prevent overfitting. It is recommended to use a small weight decay value (e.g., 1e-4).\n",
    "- ``Activation function (ReLU, Leaky ReLU, Tanh, etc)``: Different activation functions may lead to different convergence speed and performance.\n",
    "- ``Dropout rate``: Dropout is a regularization technique to prevent overfitting. It is recommended to use a small dropout rate (e.g., 0.2).\n",
    "- ...\n",
    "\n",
    "Please implement a grid search algorithm to find the best set of hyperparameters and report the best validation accuracy you can get. Any hyperparameter can be tuned!\n",
    "# Grid search\n",
    "batch_size = 64\n",
    "learning_rate = [..., ..., ...]\n",
    "epochs = 10\n",
    "input_dim = 1\n",
    "hidden_dim_list = [4, 8]    # to save time, don't tune this\n",
    "output_dim = ...            # TODO: define the output dimension\n",
    "act_fn = [..., ..., ...]    # refer to https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
    "\n",
    "# TODO: and all other hyperparameters you want to tune, e.g., dropout\n",
    "# dropout = [..., ..., ...]\n",
    "# optimizers = [..., ..., ...]   # refer to https://pytorch.org/docs/stable/optim.html\n",
    "\n",
    "loss_fn = ...       # refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_history = None\n",
    "for lr in learning_rate:\n",
    "    for af in act_fn:\n",
    "        print(...)\n",
    "        # TODO: implement the grid search\n",
    "        raise NotImplementedError\n",
    "\n",
    "print(f\"Best accuracy: {best_accuracy}\")\n",
    "print(f\"Best model: {best_model}\")\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_metrics(best_history[0], best_history[1], xlabel=\"Epoch\", ylabel=\"Loss\", title=\"Loss\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_metrics(best_history[2], best_history[3], xlabel=\"Epoch\", ylabel=\"Accuracy\", title=\"Accuracy\")\n",
    "### Step 5 Confusion Matrix\n",
    "With your best performing model, plot a confusion matrix showing which digits were misclassified, and what they were misclassified as. What numbers are frequently confused with one another by your model?\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = best_model\n",
    "model.eval()  # set model to evaluation mode\n",
    "\n",
    "# TODO: implement the confusion matrix\n",
    "raise NotImplementedError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
