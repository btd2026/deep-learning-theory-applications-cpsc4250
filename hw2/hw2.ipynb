{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "\n",
    "1. We discussed in class the similarity between PCA and an autoencoder. Show that if a linear mapping of data to k dimensions is penalized for reconstruction error, then it results in something equivalent to PCA.\n",
    "\n",
    "PCA's goal is to maximize variance. \n",
    "\n",
    "Let's look at the reconstructure error minimizing funciton used for Undercomplete Autoencoders:\n",
    "\n",
    "$$w = \\arg\\min_{w} \\mathbb{E}\\left[\\|x - w^T x\\|^2\\right]$$\n",
    "\n",
    "$$w = \\arg\\min_{w} \\mathbb{E}\\left[\\|x - w^T x\\|\\|x - w^T x\\|\\right]$$\n",
    "\n",
    "This expected reconstruction error can be written as :\n",
    "\n",
    "$$\\mathbb{E} = \\sum_{i=1}^{n} || x - \\tilde{x} || ^2$$\n",
    "\n",
    "Which is equal to the MSE that is use in PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Suppose you want to train an autoencoder to produce embeddings where clusters in the data are clearly separated, but you are not given data labels. Devise a method to do this. Provide the regularizations, loss functions and methods needed to do this.\n",
    "\n",
    "I would use PHATE for the regularization since phate reduces high dimensional data to 2D. I would use the loss function that includes a geometric loss term to encourage pairwise distances in the embedding to match original distances:\n",
    "\n",
    "$$\n",
    "L(w) = \\|\\hat{x} - x\\|^2 + \\|\\hat{y} - y\\|^2 + \\lambda\\left(\\|f(x) - f(y)\\| - d(x,y)\\right)^2\n",
    "$$\n",
    "\n",
    "Here the first two terms are reconstruction errors and the last term penalizes deviations between the distance in feature space and the original informational distance $d(x,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Suppose you want to train an autoencoder to produce embeddings where you can easily detect outliers in the test data (that are not present in the training data). Provide the regularizations, loss functions and methods needed to do this\n",
    "\n",
    "I would use Contrastive Learning as the method. This method uses data augmentation to create positive and negative samples of data and will cause outliers to be pushed away from the non-outlier data.\n",
    "\n",
    "\n",
    "For the Loss Function I would use InfoNCE since it is also a contrastive loss function + MSE for the reconstruction term since we are training an autoencoder and not just an encoder (InfoNCE us mainly for encoders and doesn't account for the decoding process). Then L1 Regularization since that will allow for sparcity. \n",
    "\n",
    "This process will ultimatley allow us to easily detect outliers since they will be separated from the rest of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "1. What similarities and differences are there between a denoising autoencoder and a variational autoencoder? How is a variational autoencoder penalized and trained differently?\n",
    "\n",
    "DAE encodes by adding noise to corrupt data and reduce it to a code. It decodes by trying to reconstruct the data from the code and is penalized by this reconstruction error.\n",
    "\n",
    "VAE encodes to a guassian approximation using a mean and standard deviation sigma. The decoder tries to map the gaussian latent space back to the original data. The loss used for regularizing here is KL Divergence and the reconstruction error of the latent vector distribution which together are also know as the ELBO.\n",
    "\n",
    "Similarities: Both DAE and VAE use a latent code to handle the encoding and decoding of data and they both use reconstuction error in some form to regularize the reconstructed data.\n",
    "\n",
    "Differences: The VAE uses the Evidence Lower Bound (ELBO) which consistes of the reconstruction error + KL Digergence since it deals with approximate of data points in constrast with DAE which uses exact datapoint. Additonally, VAE can generate data since they map to unit gaussian approximations. This makes it to that the gaussians overlap and you can generate a variant at any point between original data point\n",
    "\n",
    "2. In a variational autoencoder trained on $n$ points, suppose that for the datapoint $x$, during training the reconstructed point is $\\hat{x}$. Show that $\\hat{x}$ incurs approximately the same penalty under the following two penalties:\n",
    "\n",
    "Mean squared error penalty\n",
    "\n",
    "$\n",
    "\\frac{1}{n} |x - \\hat{x}|^{2}\n",
    "$\n",
    "\n",
    "Negative log‑likelihood penalty \n",
    "$ -\\log(p(x)) $ , where  \n",
    "\n",
    "$ p(x)\\propto e^{\\!\\left(-\\frac{\\|x-\\hat{x}\\|^{2}}{\\sigma^{2}}\\right)} $, i.e.\n",
    "\n",
    "\n",
    "3. Which penalty (MSE or $− log(p(x))$) would penalize a point more if $p(x) = λe^{−λx}$ (i.e., the exponential distribution)?\n",
    "\n",
    "$$- \\log(p(x)) = -\\log λe^{−λx} = -\\log{λe} + λx  $$ which makes negative log increase linearly.\n",
    "\n",
    "MSE is a quadratic loss function since $\n",
    "\\frac{1}{n} |x - \\hat{x}|^{2}\n",
    "$\n",
    "\n",
    "So the with $p(x) = λe^{−λx}$ , MSE would penalize a point more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
